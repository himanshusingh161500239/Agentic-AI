{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59466ab",
   "metadata": {},
   "source": [
    "<h3>AutoGen Core</h3>\n",
    "\n",
    "This is agnostic to the underlying Agent framework\n",
    "\n",
    "We can use AutoGen AgentChat, or we can use something else; it's an Agent interaction framework.\n",
    "\n",
    "From that point of view, it's positioned similarly to LangGraph.\n",
    "\n",
    "<h4>The fundamental principle</h4>\n",
    "Autogen Core decouples an agent's logic from how messages are delivered.\n",
    "The framework provides a communication infrastructure, along with agent lifecycle, and the agents are responsible for their own work.\n",
    "\n",
    "The communication infrastructure is called a Runtime.\n",
    "\n",
    "There are 2 types: Standalone and Distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d536f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
    "from autogen_core import SingleThreadedAgentRuntime\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e0992",
   "metadata": {},
   "source": [
    "<h3>Define the Message object</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6438e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    content:str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c7200",
   "metadata": {},
   "source": [
    "<h3>Define the Agent</h3>\n",
    "\n",
    "A subclass of RoutedAgent.\n",
    "\n",
    "Every Agent has an Agent ID which has 2 components:<br>\n",
    "<code>agent.id.type</code> describes the kind of agent it is<br>\n",
    "<code>agent.id.key</code> gives it its unique identifier<br>\n",
    "\n",
    "Any method with the <code>@message_handler</code> decorated will have the opportunity to receive messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11325b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"Simple\")\n",
    "\n",
    "    @message_handler\n",
    "    async def on_my_message(self, message:Message, ctx: MessageContext) -> Message:\n",
    "        return Message(content=f\"This is {self.id.type}-{self.id.key}. You said '{message.content}' and I disagree.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d6157",
   "metadata": {},
   "source": [
    "<h3>create a Standalone runtime and register our agent type</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28717515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentType(type='simple_agent')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime = SingleThreadedAgentRuntime()\n",
    "await SimpleAgent.register(runtime, \"simple_agent\", lambda: SimpleAgent())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cea5f",
   "metadata": {},
   "source": [
    "<h3>Start the runtime and send a message</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d63c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d45824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> This is simple_agent-default. You said 'Well hi there!' and I disagree.\n"
     ]
    }
   ],
   "source": [
    "agent_id = AgentId(\"simple_agent\", \"default\")\n",
    "response = await runtime.send_message(Message(\"Well hi there!\"), agent_id)\n",
    "print(\">>>\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddaf73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad25bc",
   "metadata": {},
   "source": [
    "<h3>Let's use Agentchat Assistant</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccdfc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLLMAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"LLMAgent\")\n",
    "        model_client = OpenAIChatCompletionClient(\n",
    "            model=\"gemini-2.5-flash-lite\",\n",
    "            api_key=os.getenv('GEMINI_ACCESS_KEY'),\n",
    "            model_info={\n",
    "                \"vision\": False,\n",
    "                \"function_calling\": True,\n",
    "                \"json_output\": False,  # or True if you expect structured responses\n",
    "                \"family\": \"gemini-2.5-flash\"\n",
    "            },\n",
    "        )\n",
    "        self._delagate = AssistantAgent(\"LLMAgent\",model_client=model_client)\n",
    "    \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self,message:Message, ctx:MessageContext) ->Message:\n",
    "        print(f\"{self.id.type} received message: {message.content}\")\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delagate.on_messages([text_message],ctx.cancellation_token)\n",
    "        reply=response.chat_message.content\n",
    "        print(f\"{self.id.type} responded: {reply}\")\n",
    "        return Message(content=reply)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68bfb647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentType(type='LLMAgent')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_core import SingleThreadedAgentRuntime\n",
    "\n",
    "runtime=SingleThreadedAgentRuntime()\n",
    "await SimpleAgent.register(runtime, \"simple_agent\",lambda:SimpleAgent())\n",
    "await MyLLMAgent.register(runtime,\"LLMAgent\",lambda:MyLLMAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425fa633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMAgent received message: Hi there!\n",
      "LLMAgent responded: Hi there! How can I help you today? TERMINATE\n",
      ">>>>>>>>>>> Hi there! How can I help you today? TERMINATE\n",
      ">>>>>>>>>>> This is simple_agent-default. You said 'Hi there! How can I help you today? TERMINATE' and I disagree.\n",
      "LLMAgent received message: This is simple_agent-default. You said 'Hi there! How can I help you today? TERMINATE' and I disagree.\n",
      "LLMAgent responded: I apologize if my previous response was unhelpful or incorrect. I am still under development and learning to better understand and respond to user requests. Could you please clarify what you disagree with specifically, or tell me what you were expecting? I want to make sure I'm providing the best possible assistance.\n",
      ">>>>>>>>>>> I apologize if my previous response was unhelpful or incorrect. I am still under development and learning to better understand and respond to user requests. Could you please clarify what you disagree with specifically, or tell me what you were expecting? I want to make sure I'm providing the best possible assistance.\n"
     ]
    }
   ],
   "source": [
    "runtime.start()\n",
    "response = await runtime.send_message(Message(\"Hi there!\"),AgentId(\"LLMAgent\",\"default\"))\n",
    "print(\">>>>>>>>>>>\",response.content)\n",
    "response = await runtime.send_message(Message(response.content),AgentId(\"simple_agent\",\"default\"))\n",
    "print(\">>>>>>>>>>>\",response.content)\n",
    "response = await runtime.send_message(Message(response.content),AgentId(\"LLMAgent\",\"default\"))\n",
    "print(\">>>>>>>>>>>\",response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87607674",
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f901704",
   "metadata": {},
   "source": [
    "<h3>3 agents interactions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b48a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player1Agent(RoutedAgent):\n",
    "    def __init__(self,name:str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            api_key=os.getenv('GEMINI_ACCESS_KEY'),\n",
    "            model_info={\n",
    "                \"vision\": False,\n",
    "                \"function_calling\": True,\n",
    "                \"json_output\": False,  # or True if you expect structured responses\n",
    "                \"family\": \"gemini-1.5-flash\"\n",
    "            },\n",
    "        )\n",
    "        self._delagate = AssistantAgent(name,model_client=model_client)\n",
    "    \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self,message:Message, ctx:MessageContext) ->Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delagate.on_messages([text_message],ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "\n",
    "\n",
    "class Player2Agent(RoutedAgent):\n",
    "    def __init__(self,name:str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(\n",
    "            model=\"gemini-2.5-flash-lite\",\n",
    "            api_key=os.getenv('GEMINI_ACCESS_KEY'),\n",
    "            model_info={\n",
    "                \"vision\": False,\n",
    "                \"function_calling\": True,\n",
    "                \"json_output\": False,  # or True if you expect structured responses\n",
    "                \"family\": \"gemini-2.5-flash-lite\"\n",
    "            },\n",
    "        )\n",
    "        self._delagate = AssistantAgent(name,model_client=model_client)\n",
    "    \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self,message:Message, ctx:MessageContext) ->Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delagate.on_messages([text_message],ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b18a9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE = \"You are judging a game of rock, paper, scissors. The players have made these choices:\\n\"\n",
    "\n",
    "class RockPaperScissorsAgent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            api_key=os.getenv('GEMINI_ACCESS_KEY'),\n",
    "            model_info={\n",
    "                \"vision\": False,\n",
    "                \"function_calling\": True,\n",
    "                \"json_output\": False,  # or True if you expect structured responses\n",
    "                \"family\": \"gemini-2.5-flash\"\n",
    "            },\n",
    "        )\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        instruction = \"You are playing rock, paper, scissors. Respond only with the one word, one of the following: rock, paper, or scissors.\"\n",
    "        message = Message(content=instruction)\n",
    "        inner_1 = AgentId(\"player1\", \"default\")\n",
    "        inner_2 = AgentId(\"player2\", \"default\")\n",
    "        response1 = await self.send_message(message, inner_1)\n",
    "        response2 = await self.send_message(message, inner_2)\n",
    "        result = f\"Player 1: {response1.content}\\nPlayer 2: {response2.content}\\n\"\n",
    "        judgement = f\"{JUDGE}{result}Who wins?\"\n",
    "        message = TextMessage(content=judgement, source=\"user\")\n",
    "        response = await self._delegate.on_messages([message], ctx.cancellation_token)\n",
    "        return Message(content=result + response.chat_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb56adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = SingleThreadedAgentRuntime()\n",
    "await Player1Agent.register(runtime, \"player1\", lambda: Player1Agent(\"player1\"))\n",
    "await Player2Agent.register(runtime, \"player2\", lambda: Player2Agent(\"player2\"))\n",
    "await RockPaperScissorsAgent.register(runtime, \"rock_paper_scissors\", lambda: RockPaperScissorsAgent(\"rock_paper_scissors\"))\n",
    "runtime.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd915a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1: rock\n",
      "TERMINATE\n",
      "\n",
      "Player 2: rock\n",
      "TERMINATE\n",
      "It's a draw!\n"
     ]
    }
   ],
   "source": [
    "agent_id = AgentId(\"rock_paper_scissors\", \"default\")\n",
    "message = Message(content=\"go\")\n",
    "response = await runtime.send_message(message, agent_id)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f08edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
